{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel methods for regression\n",
    "\n",
    "Kernel-based regression combine linear regression techniques with the kernel trick.\n",
    "They learn a function in the space induced by the respective kernel and the data.\n",
    "For non-linear kernels, this corresponds to a non-linear function in the original space.\n",
    "\n",
    "In this notebook, we will explore the capabilities of three of them: Suppert Vector Regression (SVR), Kernel Ridge Regression (KRR), and Gaussian Processes (GP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, svm\n",
    "\n",
    "# inline plots\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Kernel expansion and basis functions: Intuition\n",
    "\n",
    "A linear regressor fits the following model to the observed data:\n",
    "\n",
    "$$f(x) = w^\\top x$$\n",
    "\n",
    "This is the underlying function that is assumed to have generated the data, and the true data typically contains additive noise, $w^\\top x + \\epsilon$.\n",
    "\n",
    "## Kernel-based function\n",
    "\n",
    "The data model adopted by kernel-based regression techniques assumes the data fits a linear model in the feature space.\n",
    "By exploiting the representer theorem kernel trick, this model can be cast back into the input space as the following \"kernel expansion\":\n",
    "\n",
    "$$f(x) = \\sum_{i=1}^n \\alpha_i \\kappa(x, x_i)$$\n",
    "\n",
    "Here, $\\kappa(\\cdot,\\cdot)$ is the \"kernel function\", $\\{x_i\\}$, for $i=1,2,\\dots,n$, represent the data that were used for training the model, and the coefficients $\\alpha_i$ are called \"expansion coefficients\".\n",
    "\n",
    "First, we will explore the the type of functions that can be modeled in this way\n",
    "We will start with the Gaussian (or \"[RBF](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.rbf_kernel.html)\") kernel:\n",
    "\n",
    "$$\\kappa(x, x_i) = \\exp\\left(-\\gamma \\Vert x - x_i \\Vert^2\\right)$$\n",
    "\n",
    "The parameter $gamma$ indicates the \"spikiness\" of the kernel and hence allows to control the smoothness of the resulting function.\n",
    "\n",
    "Let us plot a single basis function $\\kappa(x, x_i)$ with $x_i$ fixed at the origin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "x_i = np.array(0).reshape(-1,1)\n",
    "x = np.linspace(-5,5,1000).reshape(-1,1)\n",
    "gamma = 1\n",
    "\n",
    "K = rbf_kernel(x,x_i,gamma=gamma)\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(x,K)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now choose a different $x_i$, for instance located at $x=2$ and plot the corresponding basis function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_i = np.array(2).reshape(-1,1)\n",
    "\n",
    "K = rbf_kernel(x,x_i,gamma=gamma)\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(x,K)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel expansion\n",
    "\n",
    "The complete kernel expansion $f(x) = \\sum_{i=1}^n \\alpha_i \\kappa(x, x_i)$ is obtained by weighing the individual basis functions with coefficients $\\alpha_i$ and summing the result.\n",
    "A kernel expansion with 4 \"bases\" $x_i$ is constructed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_i = np.array([-2, 0, 1, 2]).reshape(-1,1)\n",
    "alpha_i = np.array([1, 4, 2, 2]).reshape(-1,1)\n",
    "\n",
    "K = rbf_kernel(x,x_i,gamma=gamma)\n",
    "\n",
    "print(\"Dimensions of K:\"+str(K.shape))\n",
    "print(\"Dimensions of alpha_i:\"+str(alpha_i.shape))\n",
    "\n",
    "f = np.dot(K,alpha_i)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(x,alpha_i[0]*K[:,0],color='b')\n",
    "plt.plot(x,alpha_i[1]*K[:,1],color='r')\n",
    "plt.plot(x,alpha_i[2]*K[:,2],color='g')\n",
    "plt.plot(x,alpha_i[3]*K[:,3],color='y')\n",
    "plt.plot(x,f,color='k',linestyle='--',linewidth=3)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.grid(True)\n",
    "plt.title('Kernel expansion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By modifying the `gamma` parameter we obtain functions with different smoothness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 10\n",
    "\n",
    "K = rbf_kernel(x,x_i,gamma=gamma)\n",
    "\n",
    "f = np.dot(K,alpha_i)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(x,alpha_i[0]*K[:,0],color='b')\n",
    "plt.plot(x,alpha_i[1]*K[:,1],color='r')\n",
    "plt.plot(x,alpha_i[2]*K[:,2],color='g')\n",
    "plt.plot(x,alpha_i[3]*K[:,3],color='y')\n",
    "plt.plot(x,f,color='k',linestyle='--',linewidth=3)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.grid(True)\n",
    "plt.title('Kernel expansion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different kernels\n",
    "\n",
    "In practice, kernel functions reperesent dissimilarity measures.\n",
    "Apart from the standard RBF kernel many more kernels are used, such as the polynomial kernel, linear kernel, etc.\n",
    "An overview of the kernels included in the `metrics` section of scikit learn can be found at https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics.pairwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gaussian process module of scikit learn includes some advanced kernels, such as the [rational quadratic kernel](http://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.RationalQuadratic.html), the [\"white noise\" kernel](http://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.WhiteKernel.html), etc.\n",
    "\n",
    "An overview can be found at https://scikit-learn.org/stable/modules/gaussian_process.html#gp-kernels\n",
    "\n",
    "In the following code block we visualize one basis function of [the periodic \"ExpSineSquared\" kernel](http://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.ExpSineSquared.html), which is defined as\n",
    "\n",
    "$$\\kappa(x, x_i) = \\exp\\left(-2 \\left(\\frac{\\sin\\left(\\frac{\\pi}{p} d(x, x_i)\\right)}{l}\\right) ^ 2\\right)$$\n",
    "\n",
    "where $l$ is the length scale ($l=1/\\gamma$), $p$ is a periodicity parameter, and $d(\\cdot,\\cdot)$ is a distance measure (the Euclidean distance by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process.kernels import WhiteKernel, ExpSineSquared\n",
    "\n",
    "x_i = np.array(0).reshape(-1,1)\n",
    "x = np.linspace(-5,5,1000).reshape(-1,1)\n",
    "\n",
    "length_scale = .5\n",
    "periodicity = np.pi\n",
    "kernel = ExpSineSquared(length_scale=length_scale,periodicity=periodicity )\n",
    "\n",
    "K = kernel.__call__(x,x_i)\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(x,K)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combinations of kernels\n",
    "\n",
    "Kernel functions can be combined in several ways to produce more complex basis functions.\n",
    "For instance, the summation or multiplication of two kernel functions produces a new kernel function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1: Combinations of kernels\n",
    "---\n",
    "\n",
    "Kernel functions can be combined in several ways to produce new kernel functions.\n",
    "For instance, the summation or multiplication of two kernel functions produces a new kernel function.\n",
    "\n",
    "1. Write a function to calculate a kernel that is the multiplication of the `ExpSineSquared` kernel and a kernel of your choice (RBF, linear, poly, etc.).\n",
    "2. Calculate and plot the kernel expansion $f(x) = \\sum_{i=1}^n \\alpha_i \\kappa(x, x_i)$:\n",
    "    1. Choose 4 or 5 one-dimensional \"bases\" $x_i$.\n",
    "    2. Generate the corresponding coefficients $\\alpha_i$ randomly (with a fixed random seed).\n",
    "    3. Calculate and plot the kernel expansion $f(x) = \\sum_{i=1}^n \\alpha_i \\kappa(x, x_i)$ for a range of x values that includes at least the bases.\n",
    "    4. [Optional] Plot the individual basis functions corresponding to each base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Kernel function\n",
    "def my_kernel(X1, X2):\n",
    "    \"\"\"\n",
    "    Computes the linear kernel between two sets of vectors.\n",
    "    Args:\n",
    "        X1 - an n1xd matrix with vectors x1_1,...,x1_n1 in the rows\n",
    "        X2 - an n2xd matrix with vectors x2_1,...,x2_n2 in the rows\n",
    "    Returns:\n",
    "        matrix of size n1xn2, K(x1_i,x2_j) in position i,j\n",
    "    \"\"\"\n",
    "    \n",
    "    # to be completed\n",
    "    K = \n",
    "    \n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Kernel expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Regression with SVR and KRR\n",
    "\n",
    "Kernel methods for regression apply linear regression techniques in the feature space. We will first run a simple linear regression experiment on 1D data generated from a linear model.\n",
    "\n",
    "## Data generation, linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed\n",
    "np.random.seed(seed=0)\n",
    "\n",
    "# generate toy data\n",
    "n_data = 100\n",
    "w = 0.2\n",
    "b = 2\n",
    "X = np.random.normal(0,2,n_data)\n",
    "noise = np.random.normal(0,0.1,n_data)\n",
    "y = w*X + b + noise\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(X,y,s=20)\n",
    "plt.title('Linear toy data')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data for later use\n",
    "linear_data = {'X': X.reshape(-1, 1), 'y': y.reshape(-1, 1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear estimator\n",
    "\n",
    "We now run the linear least squares estimator with l2-norm regularization, from the [`linear_model.LinearRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) module, and we plot the data and the regression solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(linear_data['X'], linear_data['y'])\n",
    "\n",
    "# Test the regressor\n",
    "X_test = np.linspace(-5,5,100).reshape(-1,1)\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "h_data = plt.scatter(linear_data['X'],linear_data['y'],s=20,label='Data')\n",
    "h_line, = plt.plot(X_test,y_pred,color='r',label='Linear regression')\n",
    "plt.title('Linear regression')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(True)\n",
    "plt.legend(handles=[h_data, h_line])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, in this case we obtain an excellent fit. The (hyper)plane learned by the estimator corresponds to the linear model $f(x) = w^\\top x + b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear model (toy data)\n",
    "\n",
    "Next, we generate some data according to a nonlinear 1D model, in this case the `sinc` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed\n",
    "np.random.seed(seed=0)\n",
    "\n",
    "n_data = 200\n",
    "X = np.random.normal(0,2,n_data)\n",
    "noise = np.random.normal(0,0.05,n_data)\n",
    "y = np.sinc(X) + noise\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.scatter(X,y,s=20)\n",
    "plt.title('Noisy sinc')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data for later use\n",
    "sinc_data = {'X': X.reshape(-1, 1), 'y': y.reshape(-1, 1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to fit the linear model $f(x) = w^\\top x + b$ to these data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(sinc_data['X'], sinc_data['y'])\n",
    "\n",
    "# Test the regressor\n",
    "X_test = np.linspace(-5,5,100).reshape(-1,1)\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "h_data = plt.scatter(sinc_data['X'],sinc_data['y'],s=20,label='Data')\n",
    "h_line, = plt.plot(X_test,y_pred,color='r',label='Linear regression')\n",
    "plt.title('Linear regression')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(True)\n",
    "plt.legend(handles=[h_data, h_line])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the solution of the linear regression does not capture any of the structure in the data apart from, most probably, the data mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression on a local subset\n",
    "\n",
    "We could, however, estimate a linear model on a local subset of the data. The next code snippet selects the data that lie in $x \\in[0,1]$ and performs linear regression on these data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# select data between x=0 and x=1\n",
    "\n",
    "idx = np.logical_and(sinc_data['X'] >= 0, sinc_data['X'] <= 1)\n",
    "\n",
    "X = sinc_data['X'][idx].reshape(-1,1)\n",
    "y = sinc_data['y'][idx].reshape(-1,1)\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(X,y)\n",
    "\n",
    "# Test the regressor\n",
    "X_test = np.linspace(0,1,100).reshape(-1,1)\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "h_data = plt.scatter(sinc_data['X'],sinc_data['y'],s=20,label='Data')\n",
    "h_line, = plt.plot(X_test,y_pred,color='r',label='Linear regression')\n",
    "plt.title('Linear regression')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(True)\n",
    "plt.legend(handles=[h_data, h_line])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data shows nonlinear structure, a linear regressor may be useful at most to model the data at a local level. Instead of exploring this idea (which could be exploited to construct \"piecewise linear\" regression), we will focus on introducing kernel functions in the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear regression with SVR and KRR\n",
    "\n",
    "We will now perform nonlinear regression using the [SVR](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html) and [KRR](http://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html) algorithms.\n",
    "As we've seen, these algorithms aim to find suitable coefficients $\\alpha_i$ for the model $f(x) = \\sum_{i=1}^n \\alpha_i \\kappa(x, x_i)$ by minimizing the prediction error according to a given criterion (L2-norm, L1-norm, $\\epsilon$-insensitive,...).\n",
    "\n",
    "First, we apply both algorithms on the noisy sinc data, using some fixed choice for the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "# train the SVR. gamma ~ 1/lengthscale\n",
    "clf_svr = SVR(kernel='rbf', C=.1, gamma=1)\n",
    "clf_svr.fit(sinc_data['X'], sinc_data['y'].ravel())\n",
    "\n",
    "# train the KRR\n",
    "clf_krr = KernelRidge(kernel='rbf', alpha=5, gamma=1)\n",
    "clf_krr.fit(sinc_data['X'], sinc_data['y'])\n",
    "\n",
    "# Test the regressors\n",
    "X_test = np.linspace(-5,5,100).reshape(-1,1)\n",
    "y_pred_svr = clf_svr.predict(X_test)\n",
    "y_pred_krr = clf_krr.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.scatter(sinc_data['X'],sinc_data['y'],s=20,label='Data')\n",
    "plt.plot(X_test,y_pred_svr,color='r',label='SVR')\n",
    "plt.plot(X_test,y_pred_krr,color='g',label='KRR')\n",
    "plt.title('Linear regression')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have a look at the support vectors found and their coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Indices of support vectors:\")\n",
    "print(clf_svr.support_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coefficients:\")\n",
    "print(clf_svr.dual_coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of the support vectors\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.scatter(sinc_data['X'],sinc_data['y'],s=20,label='Data')\n",
    "plt.scatter(sinc_data['X'][clf_svr.support_],sinc_data['y'][clf_svr.support_],\\\n",
    "            s=60,marker='x',label='Support Vectors')\n",
    "plt.title('Support vectors')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter optimization by Grid Search and Cross-validation\n",
    "\n",
    "Instead of fixing the parameters, we can apply the [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) function to perform a grid search of candidate parameters and select the best set using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf_svr = GridSearchCV(SVR(kernel='rbf', gamma=0.5), cv=5,\n",
    "                   param_grid={\"C\": [1e-1, 1e0, 1e1, 1e2],\n",
    "                               \"epsilon\": [0.05, 0.1, 0.5],\n",
    "                               \"gamma\": np.logspace(-2, 2, 5)})\n",
    "\n",
    "clf_krr = GridSearchCV(KernelRidge(kernel='rbf', gamma=0.5), cv=5,\n",
    "                  param_grid={\"alpha\": [1e-1, 1e0, 0.1, 1e-2, 1e-3],\n",
    "                              \"gamma\": np.logspace(-2, 2, 5)})\n",
    "\n",
    "# train classifiers\n",
    "clf_svr.fit(sinc_data['X'], sinc_data['y'].ravel())\n",
    "clf_krr.fit(sinc_data['X'], sinc_data['y'])\n",
    "\n",
    "# Test the regressors\n",
    "X_test = np.linspace(-5,5,500).reshape(-1,1)\n",
    "y_pred_svr = clf_svr.predict(X_test)\n",
    "y_pred_krr = clf_krr.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.scatter(sinc_data['X'],sinc_data['y'],s=20,label='Data')\n",
    "plt.plot(X_test,y_pred_svr,color='r',label='SVR')\n",
    "plt.plot(X_test,y_pred_krr,color='g',label='KRR')\n",
    "plt.title('Linear regression')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best parameters for SVR: '+ str(clf_svr.best_params_))\n",
    "print('Best parameters for KRR: '+ str(clf_krr.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf_svr.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2: Motorcycle Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/motor.csv',delimiter=';')\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(df.times,df.accel,s=20)\n",
    "plt.title('Motorcycle Data')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Acceleration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Train both regression techniques (SVR and KRR) on the motorcycle data set. Determine the best hyperparameters by running the GridSearchCV optimization technique.\n",
    "\n",
    "1. Learn the optimal hyperparameters.\n",
    "2. Produce predictions and plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Learn the optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Produce predictions and plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. Gaussian Process Regression\n",
    "\n",
    "Gaussian Process Regression (GPR) is a Bayesian kernel-based technique that allows to perform nonlinear regression and thus to characterize nonlinear relationships between variables.\n",
    "When producing predictions for a new input, GPR outputs the complete probability distribution of the estimated output, which is a Gaussian centered around the \"prediction mean\".\n",
    "Interestingly, the prediction mean of KRR and GPR are identical. The \"prediction variance\" calculated by GPR yields an uncertainty interval for each prediction.\n",
    "\n",
    "A Gaussian process is an immediate generalization of the multivariate normal to infinite dimensions.\n",
    "It is defined as an infinite collection of random variables, with any marginal subset having a Gaussian distribution.\n",
    "Thus, the marginalization property is explicit in its definition.\n",
    "Another way of thinking about an infinite vector is as a function. When we write a function that takes continuous values as inputs, we are essentially implying an infinite vector that only returns values (indexed by the inputs) when the function is called upon to do so.\n",
    "By the same token, this notion of an infinite-dimensional Gaussian represented as a function allows us to work with them computationally: we are never required to store all the elements of the Gaussian process, only to calculate them on demand.\n",
    "\n",
    "Gaussian processes are Bayesian techniques: After specifying a model for the data, a prior on the functional form is chosen. Then, the data is observed.\n",
    "Finally, the posterior distribution of the function to estimate is calculated using Bayes' rule.\n",
    "\n",
    "### Selecting a prior\n",
    "\n",
    "The prior of a Gaussian process is encoded in the kernel function, which represents the covariance between any two output points. Below we select a Gaussian (RBF) prior with length scale 1, and we draw several functions that correspond to this covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "# draw samples from the unconstrained covariance\n",
    "np.random.seed(1)\n",
    "x = np.linspace(0, 10, 100)\n",
    "\n",
    "mu = np.zeros(len(x))\n",
    "\n",
    "length_scale = 1\n",
    "kernel = RBF(length_scale)\n",
    "K = kernel.__call__(x.reshape(-1,1),x.reshape(-1,1))\n",
    "\n",
    "draws = np.random.multivariate_normal(mu, K, 5)\n",
    "\n",
    "# plot a selection of unconstrained functions\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "plt.plot(x, draws.T, '-k')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel as C\n",
    "\n",
    "# Constrain the mean and covariance with two points\n",
    "x1 = np.array([2.5, 7])\n",
    "y1 = np.cos(x1)\n",
    "\n",
    "gp_kernel = C(0.1) * RBF(1)\n",
    "\n",
    "# switch off the optimizer option for this example\n",
    "gp1 = GaussianProcessRegressor(kernel=gp_kernel,optimizer=None)\n",
    "gp1.fit(x1[:, None], y1)\n",
    "f1, sigma1 = gp1.predict(x[:, None], return_std=True)\n",
    "\n",
    "# Constrain the mean and covariance with more points\n",
    "x2 = np.linspace(0,10,5).T\n",
    "y2 = np.cos(x2)\n",
    "\n",
    "gp2 = GaussianProcessRegressor(kernel=gp_kernel,optimizer=None)\n",
    "gp2.fit(x2[:, None], y2)\n",
    "f2, sigma2 = gp2.predict(x[:, None], return_std=True)\n",
    "\n",
    "# plot\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "\n",
    "# plot a constrained function\n",
    "ax = fig.add_subplot(121)\n",
    "ax.plot(x, f1, '-', color='gray')\n",
    "ax.fill_between(x, f1 - 2 * sigma1, f1 + 2 * sigma1, color='gray', alpha=0.3)\n",
    "ax.plot(x1, y1, '.k', ms=6)\n",
    "\n",
    "# plot another constrained function\n",
    "ax = fig.add_subplot(122)\n",
    "ax.plot(x, f2, '-', color='gray')\n",
    "ax.fill_between(x, f2 - 2 * sigma2, f2 + 2 * sigma2, color='gray', alpha=0.3)\n",
    "ax.plot(x2, y2, '.k', ms=6)\n",
    "\n",
    "for ax in fig.axes:\n",
    "    ax.set_xlim(0, 10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPR on noisy sinc data\n",
    "\n",
    "At this point it is interesting to note that we have not considered any noise in the model.\n",
    "In the implementation of GPR in scikit-learn, Gaussian noise is added by adding a Gaussian component to the kernel: `kernel = RBF() + WhiteKernel()`.\n",
    "\n",
    "In the following example we will apply GPR to the noisy sinc data.\n",
    "We could search for the optimal hyperparameters by running the `GridSearchCV` utility, but GPR includes a more powerful tool.\n",
    "In particular, the hyperparameters of the kernel are optimized during fitting of `GaussianProcessRegressor` by maximizing the log-marginal-likelihood (LML) based on the passed optimizer.\n",
    "As the LML may have multiple local optima, the optimizer can be started repeatedly by specifying n_restarts_optimizer.\n",
    "The first run is always conducted starting from the initial hyperparameter values of the kernel; subsequent runs are conducted from hyperparameter values that have been chosen randomly from the range of allowed values. If the initial hyperparameters should be kept fixed, `None` can be passed as optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, WhiteKernel\n",
    "\n",
    "# Instanciate a Gaussian Process model\n",
    "gp_kernel = C(1.0, (1e-2, 1e+2)) * RBF(10, (1e-2, 1e2)) + WhiteKernel(0.1,(1e-2,1e2))\n",
    "gp = GaussianProcessRegressor(kernel=gp_kernel,\n",
    "                              n_restarts_optimizer=10)\n",
    "\n",
    "# Fit to data using Maximum Likelihood Estimation of the parameters\n",
    "gp.fit(sinc_data['X'], sinc_data['y'])\n",
    "\n",
    "print(\"GPML kernel: %s\" % gp.kernel_)\n",
    "print(\"Log-marginal-likelihood: %.3f\"\n",
    "      % gp.log_marginal_likelihood(gp.kernel_.theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the prediction on the meshed x-axis\n",
    "X_test = np.linspace(-5,5,1000).reshape(-1,1)\n",
    "#X_test = np.linspace(-10,10,1000).reshape(-1,1)\n",
    "\n",
    "y_pred, sigma = gp.predict(X_test, return_std=True)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.scatter(sinc_data['X'],sinc_data['y'],s=20,label='Data')\n",
    "plt.plot(X_test,y_pred,color='r',label='GP')\n",
    "plt.fill_between(X_test.ravel(), y_pred.ravel() - 2 * sigma, y_pred.ravel() + 2 * sigma,\n",
    "                 color='gray', alpha=0.3,label='95% confidence')\n",
    "plt.title('GPR regression')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP regression on the linear data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate a Gaussian Process model\n",
    "gp_kernel = C(1.0, (1e-2, 1e+2)) * RBF(10, (1e-2, 1e2)) + WhiteKernel(0.1,(1e-2,1e2))\n",
    "gp = GaussianProcessRegressor(kernel=gp_kernel,\n",
    "                              n_restarts_optimizer=10)\n",
    "\n",
    "# Fit to data using Maximum Likelihood Estimation of the parameters\n",
    "gp.fit(linear_data['X'], linear_data['y'])\n",
    "\n",
    "print(\"GPML kernel: %s\" % gp.kernel_)\n",
    "print(\"Log-marginal-likelihood: %.3f\"\n",
    "      % gp.log_marginal_likelihood(gp.kernel_.theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.linspace(-5,5,1000).reshape(-1,1)\n",
    "#X_test = np.linspace(-10,10,1000).reshape(-1,1)\n",
    "\n",
    "# Make the prediction on the meshed x-axis\n",
    "y_pred, sigma = gp.predict(X_test, return_std=True)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.scatter(linear_data['X'],linear_data['y'],s=20,label='Data')\n",
    "plt.plot(X_test,y_pred,color='r',label='GP')\n",
    "plt.fill_between(X_test.ravel(), y_pred.ravel() - 2 * sigma, y_pred.ravel() + 2 * sigma,\n",
    "                 color='gray', alpha=0.3,label='95% confidence')\n",
    "plt.title('GPR regression')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series prediction with Gaussian Process regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is based on Section 5.4.3 of “Gaussian Processes for Machine Learning” [RW2006].\n",
    "It illustrates an example of complex kernel engineering and hyperparameter optimization using gradient ascent on the log-marginal-likelihood.\n",
    "The data consists of the monthly average atmospheric CO2 concentrations (in parts per million by volume (ppmv)) collected at the Mauna Loa Observatory in Hawaii, between 1958 and 1997.\n",
    "The objective is to model the CO2 concentration as a function of the time t.\n",
    "\n",
    "The kernel is composed of several terms that are responsible for explaining\n",
    "different properties of the signal:\n",
    "\n",
    "- a long term, smooth rising trend is to be explained by an RBF kernel. The\n",
    "  RBF kernel with a large length-scale enforces this component to be smooth;\n",
    "  it is not enforced that the trend is rising which leaves this choice to the\n",
    "  GP. The specific length-scale and the amplitude are free hyperparameters.\n",
    "\n",
    "- a seasonal component, which is to be explained by the periodic\n",
    "  ExpSineSquared kernel with a fixed periodicity of 1 year. The length-scale\n",
    "  of this periodic component, controlling its smoothness, is a free parameter.\n",
    "  In order to allow decaying away from exact periodicity, the product with an\n",
    "  RBF kernel is taken. The length-scale of this RBF component controls the\n",
    "  decay time and is a further free parameter.\n",
    "\n",
    "- smaller, medium term irregularities are to be explained by a\n",
    "  RationalQuadratic kernel component, whose length-scale and alpha parameter,\n",
    "  which determines the diffuseness of the length-scales, are to be determined.\n",
    "  According to [RW2006], these irregularities can better be explained by\n",
    "  a RationalQuadratic than an RBF kernel component, probably because it can\n",
    "  accommodate several length-scales.\n",
    "\n",
    "- a \"noise\" term, consisting of an RBF kernel contribution, which shall\n",
    "  explain the correlated noise components such as local weather phenomena,\n",
    "  and a WhiteKernel contribution for the white noise. The relative amplitudes\n",
    "  and the RBF's length scale are further free parameters.\n",
    "\n",
    "Maximizing the log-marginal-likelihood after subtracting the target's mean\n",
    "yields the following kernel with an LML of -83.214::\n",
    "\n",
    "````\n",
    "    34.4**2 * RBF(length_scale=41.8)\n",
    "    + 3.27**2 * RBF(length_scale=180) * ExpSineSquared(length_scale=1.44,periodicity=1)\n",
    "    + 0.446**2 * RationalQuadratic(alpha=17.7, length_scale=0.957)\n",
    "    + 0.197**2 * RBF(length_scale=0.138) + WhiteKernel(noise_level=0.0336)\n",
    "````\n",
    "\n",
    "Thus, most of the target signal (34.4ppm) is explained by a long-term rising\n",
    "trend (length-scale 41.8 years). The periodic component has an amplitude of\n",
    "3.27ppm, a decay time of 180 years and a length-scale of 1.44. The long decay\n",
    "time indicates that we have a locally very close to periodic seasonal\n",
    "component. The correlated noise has an amplitude of 0.197ppm with a length\n",
    "scale of 0.138 years and a white-noise contribution of 0.197ppm. Thus, the\n",
    "overall noise level is very small, indicating that the data can be very well\n",
    "explained by the model. The figure shows also that the model makes very\n",
    "confident predictions until around 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will visualize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "def load_mauna_loa_atmospheric_co2():\n",
    "    ml_data = fetch_openml(data_id=41187, cache=False)\n",
    "    months = []\n",
    "    ppmv_sums = []\n",
    "    counts = []\n",
    "\n",
    "    y = ml_data.data[:, 0]\n",
    "    m = ml_data.data[:, 1]\n",
    "    month_float = y + (m - 1) / 12\n",
    "    ppmvs = ml_data.target\n",
    "\n",
    "    for month, ppmv in zip(month_float, ppmvs):\n",
    "        if not months or month != months[-1]:\n",
    "            months.append(month)\n",
    "            ppmv_sums.append(ppmv)\n",
    "            counts.append(1)\n",
    "        else:\n",
    "            # aggregate monthly sum to produce average\n",
    "            ppmv_sums[-1] += ppmv\n",
    "            counts[-1] += 1\n",
    "\n",
    "    months = np.asarray(months).reshape(-1, 1)\n",
    "    avg_ppmvs = np.asarray(ppmv_sums) / counts\n",
    "    return months, avg_ppmvs\n",
    "\n",
    "X, y = load_mauna_loa_atmospheric_co2()\n",
    "\n",
    "# Illustration\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "plt.scatter(X, y, c='k')\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(r\"CO$_2$ in ppm\")\n",
    "plt.title(r\"Atmospheric CO$_2$ concentration at Mauna Loa\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now apply Gaussian Process regression using the kernel given in the GPML book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process.kernels import RationalQuadratic\n",
    "\n",
    "# Kernel with parameters given in GPML book\n",
    "k1 = 66.0**2 * RBF(length_scale=67.0)  # long term smooth rising trend\n",
    "k2 = 2.4**2 * RBF(length_scale=90.0) \\\n",
    "    * ExpSineSquared(length_scale=1.3, periodicity=1.0)  # seasonal component\n",
    "# medium term irregularity\n",
    "k3 = 0.66**2 \\\n",
    "    * RationalQuadratic(length_scale=1.2, alpha=0.78)\n",
    "k4 = 0.18**2 * RBF(length_scale=0.134) \\\n",
    "    + WhiteKernel(noise_level=0.19**2)  # noise terms\n",
    "kernel_gpml = k1 + k2 + k3 + k4\n",
    "\n",
    "gp = GaussianProcessRegressor(kernel=kernel_gpml, alpha=0,\n",
    "                              optimizer=None, normalize_y=True)\n",
    "gp.fit(X, y)\n",
    "\n",
    "print(\"GPML kernel: %s\" % gp.kernel_)\n",
    "print(\"Log-marginal-likelihood: %.3f\"\n",
    "      % gp.log_marginal_likelihood(gp.kernel_.theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GP regressor obtains a log-marginal-likelihood of $-117.023$ on these data, with the predefined kernel function.\n",
    "\n",
    "Let us have a look at the predictions made using this regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = np.linspace(X.min(), X.max() + 30, 1000)[:, np.newaxis]\n",
    "y_pred, y_std = gp.predict(X_, return_std=True)\n",
    "\n",
    "# plot the prediction\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "plt.scatter(X, y, c='k')\n",
    "plt.plot(X_, y_pred)\n",
    "plt.fill_between(X_[:, 0], y_pred - y_std, y_pred + y_std,\n",
    "                 alpha=0.5, color='k')\n",
    "plt.xlim(X_.min(), X_.max())\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(r\"CO$_2$ in ppm\")\n",
    "plt.title(r\"Atmospheric CO$_2$ concentration at Mauna Loa\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final part of this example, we will let the optimizer finetune the hyperparameters of the kernel function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel with generic parameters\n",
    "k1 = 50.0**2 * RBF(length_scale=50.0)  # long term smooth rising trend\n",
    "k2 = 2.0**2 * RBF(length_scale=100.0) \\\n",
    "    * ExpSineSquared(length_scale=1.0, periodicity=1.0,\n",
    "                     periodicity_bounds=\"fixed\")  # seasonal component\n",
    "# medium term irregularities\n",
    "k3 = 0.5**2 * RationalQuadratic(length_scale=1.0, alpha=1.0)\n",
    "k4 = 0.1**2 * RBF(length_scale=0.1) \\\n",
    "    + WhiteKernel(noise_level=0.1**2,\n",
    "                  noise_level_bounds=(1e-3, np.inf))  # noise terms\n",
    "kernel = k1 + k2 + k3 + k4\n",
    "\n",
    "# Fit the Gaussian Process regressor, and optimize the hyperparameters\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=0,\n",
    "                              normalize_y=True)\n",
    "gp.fit(X, y)\n",
    "\n",
    "print(\"\\nLearned kernel: %s\" % gp.kernel_)\n",
    "print(\"Log-marginal-likelihood: %.3f\"\n",
    "      % gp.log_marginal_likelihood(gp.kernel_.theta))\n",
    "\n",
    "X_ = np.linspace(X.min(), X.max() + 30, 1000)[:, np.newaxis]\n",
    "y_pred, y_std = gp.predict(X_, return_std=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all went correctly, we should obtain a higher log-marginal-likelihood on these data after optimizing the kernel function hyperparameters.\n",
    "This solution should fit slightly better to the data, as the optimizer aims to maximize the LML.\n",
    "\n",
    "Let us have a look at the predictions made using this regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the prediction\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "plt.scatter(X, y, c='k')\n",
    "plt.plot(X_, y_pred)\n",
    "plt.fill_between(X_[:, 0], y_pred - y_std, y_pred + y_std,\n",
    "                 alpha=0.5, color='k')\n",
    "plt.xlim(X_.min(), X_.max())\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(r\"CO$_2$ in ppm\")\n",
    "plt.title(r\"Atmospheric CO$_2$ concentration at Mauna Loa\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
