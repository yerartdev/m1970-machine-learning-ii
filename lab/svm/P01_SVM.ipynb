{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "Course: Machine Learning II, Data Science Master (Universidad de Cantabria - UIMP).  \n",
    "Author: Steven Van Vaerenbergh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Support vector machines (SVMs) are a particularly powerful and flexible class of supervised algorithms for both classification and regression.\n",
    "In this section, we will develop the intuition behind support vector machines and their use in classification problems.\n",
    "\n",
    "We begin with the standard imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, svm\n",
    "\n",
    "# inline plots\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines (SVMs) are a powerful supervised learning algorithm used for **classification** or for **regression**. SVMs are a **discriminative** classifier: that is, they draw a boundary between clusters of data.\n",
    "\n",
    "Let's show a quick example of support vector classification. First we need to create a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "X, y = make_blobs(n_samples=50, centers=2,\n",
    "                  random_state=0, cluster_std=0.60)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='bwr');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear discriminative classifier would attempt to draw a straight line separating the two sets of data, and thereby create a model for classification.\n",
    "For two dimensional data like that shown here, this is a task we could do by hand.\n",
    "But immediately we see a problem: there is more than one possible dividing line that can perfectly discriminate between the two classes!\n",
    "\n",
    "We can draw them as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 3.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='bwr')\n",
    "plt.plot([0.6], [2.1], 'x', color='green', markeredgewidth=2, markersize=10)\n",
    "\n",
    "# plot dividing lines\n",
    "for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:\n",
    "    plt.plot(xfit, m * xfit + b, '-k')\n",
    "\n",
    "# set x limits\n",
    "plt.xlim(-1, 3.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are three *very* different separators which, nevertheless, perfectly discriminate between these samples.\n",
    "Depending on which you choose, a new data point (e.g., the one marked by the \"X\" in this plot) will be assigned a different label!\n",
    "\n",
    "Evidently our simple intuition of \"drawing a line between classes\" is not enough, and we need to think a bit deeper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines: Maximizing the *Margin*\n",
    "\n",
    "Support vector machines offer one way to improve on this. The intuition is this: rather than simply drawing a zero-width line between the classes, we can draw around each line a *margin* of some width, up to the nearest point.\n",
    "\n",
    "Here is an example of how this might look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 3.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='bwr')\n",
    "\n",
    "# plot divigind lines and margins (pre-calculated)\n",
    "for m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:\n",
    "    yfit = m * xfit + b\n",
    "    plt.plot(xfit, yfit, '-k')\n",
    "    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none', color='#AAAAAA', alpha=0.4)\n",
    "\n",
    "# set x limits\n",
    "plt.xlim(-1, 3.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice here that if we want to maximize this width, the middle fit is clearly the best.\n",
    "\n",
    "In support vector machines, the line that maximizes this margin is the one we will choose as the optimal model.\n",
    "Support vector machines are an example of such a *maximum margin* estimator.\n",
    "\n",
    "This is the intuition of **support vector machines**, which optimize a linear discriminant model in conjunction with a **margin** representing the perpendicular distance between the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a Support Vector Machine\n",
    "\n",
    "Let's see the result of an actual fit to this data: we will use Scikit-Learn's support vector classifier to train an SVM model on this data.\n",
    "For the time being, we will use a linear kernel and set the penalty parameter ``C`` to a very large number (which corresponds to a very low regularization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC # \"Support vector classifier\"\n",
    "clf = SVC(kernel='linear', C=1E6)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better visualize what's happening here, let us create a quick convenience function that will plot SVM decision boundaries for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
    "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # create grid to evaluate model\n",
    "    x = np.linspace(xlim[0], xlim[1], 50)\n",
    "    y = np.linspace(ylim[0], ylim[1], 50)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "    \n",
    "    # plot decision boundary and margins\n",
    "    ax.contour(X, Y, P, colors='k',\n",
    "               levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "    \n",
    "    # plot support vectors\n",
    "    if plot_support:\n",
    "        ax.scatter(model.support_vectors_[:, 0],\n",
    "                   model.support_vectors_[:, 1],\n",
    "                   s=300, linewidth=1, facecolors='none',\n",
    "                   edgecolor='black')\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='bwr')\n",
    "plot_svc_decision_function(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the dividing line that maximizes the margin between the two sets of points.\n",
    "Notice that a few of the training points just touch the margin: they are indicated by the black circles in this figure.\n",
    "These points are the pivotal elements of this fit, and are known as the *support vectors*, and give the algorithm its name.\n",
    "In Scikit-Learn, the identity of these points are stored in the ``support_vectors_`` attribute of the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.support_vectors_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key to this classifier's success is that for the fit, only the position of the support vectors matter; any points further from the margin which are on the correct side do not modify the fit!\n",
    "Technically, this is because these points do not contribute to the loss function used to fit the model, so their position and number do not matter so long as they do not cross the margin.\n",
    "\n",
    "We can see this, for example, if we plot the model learned from the first 50 points and first 100 points of this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svm(N=10, ax=None):\n",
    "    X, y = make_blobs(n_samples=200, centers=2,\n",
    "                      random_state=0, cluster_std=0.60)\n",
    "    X = X[:N]\n",
    "    y = y[:N]\n",
    "    model = SVC(kernel='linear', C=1E6)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    ax = ax or plt.gca()\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='bwr')\n",
    "    ax.set_xlim(-1, 4)\n",
    "    ax.set_ylim(-1, 6)\n",
    "    plot_svc_decision_function(model, ax)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "for axi, N in zip(ax, [50, 100]):\n",
    "    plot_svm(N, axi)\n",
    "    axi.set_title('N = {0}'.format(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the left panel, we see the model and the support vectors for 60 training points.\n",
    "In the right panel, we have doubled the number of training points, but the model has not changed: the three support vectors from the left panel are still the support vectors from the right panel.\n",
    "This insensitivity to the exact behavior of distant points is one of the strengths of the SVM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What othervariables have been calculated by the SVM?\n",
    "Let's have a look at the documentation: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Indices of support vectors: {}\\n\".format(clf.support_))\n",
    "\n",
    "print(\"Number of support vectors for each class: {}\\n\".format(clf.n_support_))\n",
    "\n",
    "print(\"Coefficients of the support vector in the decision function: {}\\n\".format(clf.dual_coef_))\n",
    "\n",
    "print(\"Weights assigned to the features (linear SVM only).: {}\\n\".format(clf.coef_))\n",
    "\n",
    "print(\"Constants in decision function: {}\\n\".format(clf.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Nonlinear SVM using Kernels\n",
    "\n",
    "Where SVM becomes extremely powerful is when it is combined with *kernels*.\n",
    "By using kernels we will project our data into higher-dimensional space defined by polynomial, Gaussian or other basis functions, and thereby we will be able to fit for nonlinear relationships with a linear classifier.\n",
    "\n",
    "To motivate the need for kernels, let's look at some data that is not linearly separable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_circles\n",
    "X, y = make_circles(100, factor=.1, noise=.1, random_state=0)\n",
    "\n",
    "clf = SVC(kernel='linear').fit(X, y)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='bwr')\n",
    "plot_svc_decision_function(clf, plot_support=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that no linear discrimination will *ever* be able to separate this data.\n",
    "But, we could project the data into a higher dimension such that a linear separator *would* be sufficient.\n",
    "\n",
    "For example, we could define the simple polynomial mapping $\\Phi(x) = \\begin{bmatrix}x_1^2\\\\x_2^2\\\\\\sqrt{2}x_1x_2\\end{bmatrix}$ and apply the support vector machine on $\\Phi(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the polynomial mapping. [:,None] is usd to convert 1D array to 2D array.\n",
    "Phi = np.concatenate((X**2, np.sqrt(2)*(X[:,0]*X[:,1])[:,None]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize this extra data dimension using a three-dimensional plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# bug fix for matplotlib 3.0.0\n",
    "%config InlineBackend.print_figure_kwargs = {'bbox_inches':None}\n",
    "\n",
    "fig = plt.figure(figsize=(12,10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# retrieve indices of zeroes and ones\n",
    "idx0 = y==0\n",
    "idx1 = y==1\n",
    "\n",
    "# initialize the viewing angles\n",
    "ax.view_init(30, 50)\n",
    "\n",
    "# plot the data\n",
    "ax.scatter(Phi[idx0,0], Phi[idx0,1], Phi[idx0,2], c='b', marker='o', s=50)\n",
    "ax.scatter(Phi[idx1,0], Phi[idx1,1], Phi[idx1,2], c='r', marker='o', s=50)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us generate an interactive plot to inspect the data more closely.\n",
    "We will use the Plotly library: https://plot.ly/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install plotly\n",
    "\n",
    "# load Plotly\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, iplot\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# initiate the Plotly Notebook mode\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data\n",
    "\n",
    "def plot_3D(X, y):\n",
    "    \n",
    "    # retrieve indices of zeroes and ones\n",
    "    idx0 = y==0\n",
    "    idx1 = y==1\n",
    "    \n",
    "    # define data set 1 plot options\n",
    "    trace0 = go.Scatter3d(x=X[idx0,0], y=X[idx0,1], z=X[idx0,2],\n",
    "        mode='markers',\n",
    "        marker=dict(size=6, color='blue', opacity=0.8)\n",
    "    )\n",
    "\n",
    "    # define data set 2 plot options\n",
    "    trace1 = go.Scatter3d(x=X[idx1,0], y=X[idx1,1], z=X[idx1,2],\n",
    "        mode='markers',\n",
    "        marker=dict(size=6, color='red', opacity=0.8)\n",
    "    )\n",
    "\n",
    "    # set aspect ratio\n",
    "    scene = dict(aspectmode=\"manual\", aspectratio=dict(x=1, y=1, z=1))\n",
    "\n",
    "    # define figure properties\n",
    "    layout = go.Layout(\n",
    "        scene=scene,\n",
    "        height=600,\n",
    "        width=900\n",
    "    )\n",
    "\n",
    "    # produce the plot\n",
    "    fig = go.Figure(data=[trace0,trace1],layout=layout)\n",
    "    iplot(fig)\n",
    "    \n",
    "plot_3D(Phi,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another simple projection could be to simply add a *radial basis function* centered on the origin (where the red cluster resides): $\\Phi(x) = \\begin{bmatrix}x_1\\\\x_2\\\\\\exp\\left(-\\left\\|\\begin{bmatrix}x_1\\\\x_2\\end{bmatrix}\\right\\|^2)\\right)\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the mapping.\n",
    "Phi = np.concatenate((X*X, np.exp(-(X ** 2).sum(1))[:,None]), axis=1)\n",
    "\n",
    "# plot\n",
    "plot_3D(Phi,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases, we can see that with the additional dimension the data becomes trivially linearly separable, by drawing a separating plane at, say, *r*=0.7 in the last example.\n",
    "\n",
    "Here we had to choose and carefully tune our projection: if we had not centered our radial basis function in the right location, we would not have seen such clean, linearly separable results.\n",
    "In general, the need to make such a choice is a problem: we would like to somehow automatically find the best basis functions to use.\n",
    "\n",
    "One strategy to this end is to compute a basis function centered at *every* point in the dataset, and let the SVM algorithm sift through the results.\n",
    "This type of basis function transformation is known as a *kernel transformation*, as it is based on a similarity relationship (or kernel) between each pair of points.\n",
    "\n",
    "A potential problem with this strategy—projecting $N$ points into $N$ dimensions—is that it might become very computationally intensive as $N$ grows large.\n",
    "However, because of a neat little procedure known as the [*kernel trick*](https://en.wikipedia.org/wiki/Kernel_trick), a fit on kernel-transformed data can be done implicitly—that is, without ever building the full $N$-dimensional representation of the kernel projection!\n",
    "This kernel trick is built into the SVM, and is one of the reasons the method is so powerful.\n",
    "\n",
    "In Scikit-Learn, we can apply kernelized SVM simply by changing our linear kernel to an RBF (radial basis function) kernel, using the ``kernel`` model hyperparameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf', C=1E6, gamma='auto')\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='bwr')\n",
    "plot_svc_decision_function(clf)\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
    "            s=300, lw=1, facecolors='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this kernelized support vector machine, we learn a suitable nonlinear decision boundary.\n",
    "This kernel transformation strategy is used often in machine learning to turn fast linear methods into fast nonlinear methods, especially for models in which the kernel trick can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the SVM model for different numbers of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_svm_circles(N=5, ax=None):\n",
    "    X, y = make_circles(500, factor=.1, noise=.1, random_state=0)\n",
    "\n",
    "    X = X[:N]\n",
    "    y = y[:N]\n",
    "    model = SVC(kernel='rbf', C=1E6, gamma='auto')\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    ax = ax or plt.gca()\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='bwr')\n",
    "    ax.set_xlim(-1.5, 1.5)\n",
    "    ax.set_ylim(-1.5, 1.5)\n",
    "    plot_svc_decision_function(model, ax)\n",
    "\n",
    "# initialize figure with subplots\n",
    "fig, ax = plt.subplots(3, 2, figsize=(12, 15),sharex=True,sharey=True)\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "for axi, N in zip(np.ravel(ax), [5, 10, 20, 50, 100, 200]):\n",
    "    \n",
    "    # generate data (same every loop)\n",
    "    X, y = make_circles(500, factor=.1, noise=.1, random_state=0)\n",
    "\n",
    "    # select data and train SVM\n",
    "    X = X[:N]\n",
    "    y = y[:N]\n",
    "    model = SVC(kernel='rbf', C=1E6, gamma='auto')\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    axi.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='bwr')\n",
    "    axi.set_xlim(-1.5, 1.5)\n",
    "    axi.set_ylim(-1.5, 1.5)\n",
    "    plot_svc_decision_function(model, axi)\n",
    "    axi.set_title('SVM for N=%d'%N)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Tuning the SVM\n",
    "\n",
    "## 3.1 Softening Margins\n",
    "\n",
    "Our discussion thus far has centered around very clean datasets, in which a perfect decision boundary exists.\n",
    "But what if your data has some amount of overlap?\n",
    "For example, you may have data like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=100, centers=2,\n",
    "                  random_state=0, cluster_std=1.2)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='bwr');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle this case, the SVM implementation has a bit of a fudge-factor which \"softens\" the margin: that is, it allows some of the points to creep into the margin if that allows a better fit.\n",
    "The hardness of the margin is controlled by a tuning parameter, most often known as $C$.\n",
    "For very large $C$, the margin is hard, and points cannot lie in it.\n",
    "For smaller $C$, the margin is softer, and can grow to encompass some points.\n",
    "\n",
    "The plot shown below gives a visual picture of how a changing $C$ parameter affects the final fit, via the softening of the margin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=100, centers=2,\n",
    "                  random_state=0, cluster_std=0.8)\n",
    "\n",
    "# initialize figure with subplots\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6),sharey=True)\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "for axi, C in zip(ax, [10.0, 0.1]):\n",
    "    # train SVM\n",
    "    model = SVC(kernel='linear', C=C).fit(X, y)\n",
    "    \n",
    "    # plot results    \n",
    "    axi.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='bwr')\n",
    "    plot_svc_decision_function(model, axi)\n",
    "    axi.scatter(model.support_vectors_[:, 0],\n",
    "                model.support_vectors_[:, 1],\n",
    "                s=300, lw=1, facecolors='none')\n",
    "    axi.set_title('C = {0:.1f}'.format(C), size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal value of the $C$ parameter will depend on your dataset, and should be tuned using cross-validation or a similar procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 The RBF kernel parameter $\\gamma$\n",
    "\n",
    "The RBF (or *Gaussian*) kernel is defined as $k(x, x') = e^{- \\gamma \\Vert x - x' \\Vert^2}$, where is expressed in terms of the Gaussian kernel width $\\sigma$ as $\\gamma = \\frac{1}{2 \\sigma^2}$. The parameter $\\gamma$ controls the smoothness of the decision boundary. In the following interactive example we will study the influence of both the parameter $\\gamma$ and the parameter $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_svm_circles(gamma=.1, C=10, ax=None):\n",
    "    X, y = make_blobs(n_samples=100, centers=2,\n",
    "                  random_state=2, cluster_std=2)\n",
    "\n",
    "    model = SVC(kernel='rbf',gamma=gamma, C=C)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    ax = ax or plt.gca()\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='bwr')\n",
    "    plot_svc_decision_function(model, ax, plot_support=True)\n",
    "\n",
    "# define parameters\n",
    "gamma_values = [0.01,0.1,1,10]\n",
    "C_values = [0.1,1,10,100]\n",
    "\n",
    "# initialize figure with subplots\n",
    "fig, ax = plt.subplots(len(gamma_values), len(C_values), figsize=(16, 16),\n",
    "                       sharex=True, sharey=True)\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "# all possible pairs of gammas and Cs\n",
    "parameters = np.array(np.meshgrid(gamma_values,C_values)).T.reshape(-1,2)\n",
    "\n",
    "for axi, gammaC in zip(np.ravel(ax), parameters):\n",
    "    gamma = gammaC[0]\n",
    "    C = gammaC[1]\n",
    "    \n",
    "    plot_svm_circles(gamma,C,ax=axi)\n",
    "    axi.set_title('$\\gamma$=%.2f, C=%.1f'%(gamma,C), size=14)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Examples with real data\n",
    "\n",
    "## Example: Iris Dataset\n",
    "As an example of support vector machines in action, let's take a look at the iris data classification problem. While these data have 4 dimensions and 3 classes, we will use a subset of the data that uses only the first two dimensions and the last two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# remove class 0 and select the first 2 dimensions\n",
    "X = X[y != 0, :2]\n",
    "y = y[y != 0]\n",
    "\n",
    "print('Size of X is: '+str(X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train the classifier on one subset of the data, called the *training* data, and we will withold a separate subset called the *test* data to test or validate the classifier's performance.\n",
    "\n",
    "Splitting the data into training and test set could be done \"manually\" as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fraction of the data to assign to the test set\n",
    "test_size = 0.2\n",
    "\n",
    "n_sample = len(X)\n",
    "\n",
    "# randomly permute the data\n",
    "np.random.seed(0)\n",
    "order = np.random.permutation(n_sample)\n",
    "X_permutated = X[order]\n",
    "y_permutated = y[order].astype(np.float)\n",
    "\n",
    "X_train = X_permutated[int(test_size * n_sample):]\n",
    "y_train = y_permutated[int(test_size * n_sample):]\n",
    "X_test = X_permutated[:int(test_size * n_sample)]\n",
    "y_test = y_permutated[:int(test_size * n_sample)]\n",
    "\n",
    "print('Size of X_train is: '+str(X_train.shape))\n",
    "print('Size of X_test is: '+str(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, we could do all of this in one line using scikit-learn's built-in function `train_test_split`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_size, random_state=0)\n",
    "\n",
    "print('Size of X_train is: '+str(X_train.shape))\n",
    "print('Size of X_test is: '+str(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of SVM kernels on Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "kernels = ('linear', 'rbf', 'poly')\n",
    "\n",
    "# fit the model\n",
    "for fig_num, kernel in enumerate(('linear', 'rbf', 'poly')):\n",
    "    clf = svm.SVC(kernel=kernel, gamma=10)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    plt.figure(fig_num)\n",
    "    plt.clf()\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, zorder=10, cmap='bwr', s=50)\n",
    "\n",
    "    plt.axis('tight')\n",
    "    x_min = X[:, 0].min()\n",
    "    x_max = X[:, 0].max()\n",
    "    y_min = X[:, 1].min()\n",
    "    y_max = X[:, 1].max()\n",
    "\n",
    "    XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n",
    "    \n",
    "    plot_svc_decision_function(clf, plot_support=False)\n",
    "\n",
    "    # evaluation\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    print('Accuracy for '+kernel+': '+str(accuracy_score(y_test, y_pred)))\n",
    "    \n",
    "    plt.title(kernel)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Iris Dataset\n",
    "\n",
    "Apply an SVM classifier on the complete 4-dimensional Iris data set (2 classes only).\n",
    "\n",
    "1. Construct the data. Choose a training size of 70% of the data.\n",
    "2. Choose a kernel and run the SVM classifier on the training data.\n",
    "3. Calculate the evaluation accuracy on the test data. Adjust the kernel parameters to maximize the accuracy.\n",
    "4. [Bonus] Make a 3D plot of the results. Use a 3D scatter plot to draw the data, and indicate misclassified test data with crosses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X is: (100, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "# 1. Data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# remove class 0 and select the first 2 dimensions\n",
    "X = X[y != 0, :]\n",
    "y = y[y != 0]\n",
    "\n",
    "print('Size of X is: '+str(X.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X_train is: (70, 4)\n",
      "Size of X_test is: (30, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_size = 0.3\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_size, random_state=0)\n",
    "\n",
    "print('Size of X_train is: '+str(X_train.shape))\n",
    "print('Size of X_test is: '+str(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=10, kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "# 2. SVM\n",
    "clf = svm.SVC(kernel='linear', gamma=10)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for linear: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# 3. Evaluate\n",
    "# > 0.7\n",
    "y_pred = clf.predict(X_test)\n",
    "print('Accuracy for linear: '+str(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. [Bonus] 3D plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Spam detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example our goal is to use an SVM to build a spam filter. Here is a sample of a legitimate email:\n",
    "\n",
    "````\n",
    "> Anyone knows how much it costs to host a web portal ?\n",
    ">\n",
    "Well, it depends on how many visitors you're expecting.\n",
    "This can be anywhere from less than 10 bucks a month to a couple of $100. \n",
    "You should checkout http://www.rackspace.com/ or perhaps Amazon EC2 \n",
    "if youre running something big..\n",
    "\n",
    "To unsubscribe yourself from this mailing list, send an email to:\n",
    "groupname-unsubscribe@egroups.com\n",
    "````\n",
    "\n",
    "And here is a sample of a spam email:\n",
    "\n",
    "````\n",
    "Do You Want To Make $1000 Or More Per Week?\n",
    "\n",
    "If you are a motivated and qualified individual - I \n",
    "will personally demonstrate to you a system that will \n",
    "make you $1,000 per week or more! This is NOT mlm.\n",
    "\n",
    "Call our 24 hour pre-recorded number to get the \n",
    "details.  \n",
    "\n",
    "000-456-789\n",
    "````\n",
    "\n",
    "The trained SVM must be capable of predicting whether or not a new email is spam. Before training the SVM, however, the text input must be preprocessed so that we can calculate a meaningful kernel. Preprocessing usually consists in cleanup tasks such as HTML removal, stemming, normalization. Then, a dictionary is constructured for all words that should be considered. Finally, the text of each email is mapped to a vector whose length is the number of dictionary words, and the i-th entry in this vector is a boolean indicating if the i-th dictionary word appears in the email. These vectors are called *word occurance vectors*.\n",
    "\n",
    "In order to focus on the machine learning task we use pre-processed training and test data to train the classifier. These sets contain the spam and non-spam emails transformed to word occurance vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "\n",
    "spam_train = loadmat('data/spamTrain.mat')\n",
    "spam_test = loadmat('data/spamTest.mat')\n",
    "\n",
    "spam_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = spam_train['X']\n",
    "X_test = spam_test['Xtest']\n",
    "y_train = spam_train['y'].ravel()\n",
    "y_test = spam_test['ytest'].ravel()\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each document has been converted to a vector with 1,899 dimensions corresponding to the 1,899 words in the dictionary. The values are binary, indicating the presence or absence of the word in the document. At this point, training and evaluation are just a matter of fitting the testing the classifer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = svm.SVC(kernel='rbf',gamma='auto')\n",
    "svc.fit(X_train, y_train)\n",
    "print('Training accuracy = {0}%'.format(np.round(svc.score(X_train, y_train) * 100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test accuracy = {0}%'.format(np.round(svc.score(X_test, y_test) * 100, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These result were obtained with the default parameters. Go ahead and change the kernel parameters in order to increase the test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Face Recognition\n",
    "\n",
    "As an example of support vector machines in action, let's take a look at the facial recognition problem.\n",
    "We will use the Labeled Faces in the Wild dataset, which consists of several thousand collated photos of various public figures.\n",
    "A fetcher for the dataset is built into Scikit-Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python -W ignore::DeprecationWarning\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "faces = fetch_lfw_people(min_faces_per_person=60)\n",
    "\n",
    "print(faces.target_names)\n",
    "print(faces.images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us analyze the dimensions of the input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# introspect the images arrays to find the shapes (for plotting)\n",
    "n_samples, h, w = faces.images.shape\n",
    "\n",
    "# for machine learning we use the 2 data directly (as relative pixel\n",
    "# positions info is ignored by this model)\n",
    "n_features = faces.data.shape[1]\n",
    "\n",
    "# the label to predict is the id of the person\n",
    "y = faces.target\n",
    "target_names = faces.target_names\n",
    "n_classes = target_names.shape[0]\n",
    "\n",
    "print(\"Total dataset size:\")\n",
    "print(\"n_samples: %d\" % n_samples)\n",
    "print(\"n_features: %d\" % n_features)\n",
    "print(\"n_classes: %d\" % n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot a few of these faces to see what we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gallery(images, titles, h, w, n_row=3, n_col=7):\n",
    "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
    "    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        plt.title(titles[i], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "\n",
    "plot_gallery(faces.data, faces.target_names[faces.target], h, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split the data into a training and testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(faces.data, faces.target,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image contains [62×47] or nearly 2914 pixels.\n",
    "We could proceed by simply using each pixel value as a feature, but often it is more effective to use some sort of preprocessor to extract more meaningful features; here we will use a principal component analysis to extract 150 principal components to feed into our support vector machine classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# PCA with whitening, which will remove the relative variance\n",
    "# scales of the components\n",
    "n_components = 150\n",
    "pca = PCA(n_components=n_components, whiten=True,\n",
    "          svd_solver='randomized', random_state=0)\n",
    "\n",
    "# Calculata and apply PCA\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "\n",
    "# Check the percentage of the variance explained by the PCs\n",
    "var_explained = pca.explained_variance_ratio_.cumsum()[n_components-1]\n",
    "print(\"Variance explained by the first %d components: %.2f\"%(n_components,var_explained))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have a look at the eigenfaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the principal directions, called the \"eigenfaces\"\n",
    "eigenfaces = pca.components_.reshape((n_components, h, w))\n",
    "\n",
    "# Plot the gallery of the most significant eigenfaces\n",
    "eigenface_titles = [\"eigenface %d\" % i for i in range(eigenfaces.shape[0])]\n",
    "plot_gallery(eigenfaces, eigenface_titles, h, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will train the SVM classifier. In order to determine the best parameters $C$ and $\\gamma$ for our SVM we can use a grid search cross-validation to explore combinations of parameters.\n",
    "Here we will adjust ``C`` (which controls the margin hardness) and ``gamma`` (which controls the size of the radial basis function kernel), and determine the best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from time import time\n",
    "\n",
    "t0 = time()\n",
    "print(\"Fitting the classifier to the training set...\")\n",
    "param_grid = {'C': [1E2, 5E2, 1E3, 5E3, 1E4],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
    "grid = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'), param_grid, cv=3)\n",
    "grid.fit(X_train_pca, y_train)\n",
    "clf = grid.best_estimator_\n",
    "\n",
    "print(\"Elapsed time: %0.3fs\"%(time() - t0))\n",
    "print(\"Best estimator found by grid search:\")\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can test our SVM classifier on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predicting people's names on the test set...\")\n",
    "\n",
    "# apply the same PCA transform as before\n",
    "X_test_pca = pca.transform(X_test)\n",
    "y_pred = clf.predict(X_test_pca)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a look at a few of the test images along with their predicted values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_title(y_pred, y_test, target_names, i):\n",
    "    pred_name = target_names[y_pred[i]].rsplit(' ', 1)[-1]\n",
    "    true_name = target_names[y_test[i]].rsplit(' ', 1)[-1]\n",
    "    return 'predicted: %s\\ntrue:      %s' % (pred_name, true_name)\n",
    "\n",
    "prediction_titles = [make_title(y_pred, y_test, target_names, i)\n",
    "                     for i in range(y_pred.shape[0])]\n",
    "\n",
    "plot_gallery(X_test, prediction_titles, h, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this small sample, our optimal estimator mislabeled a few faces.\n",
    "We can get a better sense of our estimator's performance using the classification report, which lists recovery statistics label by label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=faces.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, y_pred, labels=range(n_classes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might also display the confusion matrix between these classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# use seaborn plotting defaults\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "mat = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(7,6))\n",
    "sns.set(font_scale=1.3)\n",
    "sns.heatmap(mat.T, square=False, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=faces.target_names,\n",
    "            yticklabels=faces.target_names,\n",
    "            cmap=sns.cubehelix_palette(light=1, as_cmap=True))\n",
    "\n",
    "plt.xlabel('true label');\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helps us get a sense of which labels are likely to be confused by the estimator.\n",
    "\n",
    "For a real-world facial recognition task, in which the photos do not come pre-cropped into nice grids, the only difference in the facial classification scheme is the feature selection: you would need to use a more sophisticated algorithm to find the faces, and extract features that are independent of the pixellation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Face Recognition\n",
    "\n",
    "In the previous example we have chosen a number of principal components by hand.\n",
    "In order to find suitable parameters for the SVM, we have applied a grid search in which for each set of parameter values we have ran a cross-validation procedure to determine the fitness of the parameters.\n",
    "In this exercise you will extend the gird search to the number of principal components, using the [Pipeline class](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) from scikit-learn.\n",
    "\n",
    "1. Build a classifier that consists of a pipeline of PCA and the support vector classifier.\n",
    "2. Define a grid containing the values to consider for each of the parametres:\n",
    "  - 3 values for PCA's `n_components`\n",
    "  - 3 values for SVC's `C`\n",
    "  - 3 values for SVC's `gamma`\n",
    "3. Use GridSearchCV to find the best set of values for these 3 parameters.\n",
    "4. Apply the classifier with the best parameters to the test set and plot the `classification_report` and the `confusion_matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define grid for GridsearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Run GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Test the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine Summary\n",
    "\n",
    "We have seen here a brief intuitive introduction to the principals behind support vector machines.\n",
    "These methods are a powerful classification method for a number of reasons:\n",
    "\n",
    "- Their dependence on relatively few support vectors means that they are very compact models, and take up very little memory.\n",
    "- Once the model is trained, the prediction phase is very fast.\n",
    "- Because they are affected only by points near the margin, they work well with high-dimensional data—even data with more dimensions than samples, which is a challenging regime for other algorithms.\n",
    "- Their integration with kernel methods makes them very versatile, able to adapt to many types of data.\n",
    "\n",
    "However, SVMs have several disadvantages as well:\n",
    "\n",
    "- The scaling with the number of samples $N$ is $\\mathcal{O}[N^3]$ at worst, or $\\mathcal{O}[N^2]$ for efficient implementations. For large numbers of training samples, this computational cost can be prohibitive.\n",
    "- The results are strongly dependent on a suitable choice for the softening parameter $C$. This must be carefully chosen via cross-validation, which can be expensive as datasets grow in size.\n",
    "- The results do not have a direct probabilistic interpretation. This can be estimated via an internal cross-validation (see the ``probability`` parameter of ``SVC``), but this extra estimation is costly.\n",
    "\n",
    "With those traits in mind, SVMs are advisable once other simpler, faster, and less tuning-intensive methods have been shown to be insufficient.\n",
    "Nevertheless, if you have the CPU cycles to commit to training and cross-validating an SVM on your data, the method can lead to excellent results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources\n",
    "\n",
    "- [Python Data Science Handbook by Jake VanderPlas](https://github.com/jakevdp/PythonDataScienceHandbook). MIT License.\n",
    "- [scikit-learn Examples](http://scikit-learn.org/stable/auto_examples/). BSD License.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
