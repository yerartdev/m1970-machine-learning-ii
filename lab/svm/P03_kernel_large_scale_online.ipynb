{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large-Scale and Online Kernel Methods\n",
    "\n",
    "Course: Machine Learning II, Data Science Master (Universidad de Cantabria - UIMP).  \n",
    "Author: Steven Van Vaerenbergh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "\n",
    "# inline plots\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, let us recall the following formulae:\n",
    "\n",
    "1. Kernel expansion: $$f(x) = \\sum_{i=1}^n \\alpha_i \\kappa(x, x_i)$$\n",
    "2. Kernel ridge regression $$\\boldsymbol{\\alpha} = \\left( {\\bf K} + \\lambda {\\bf I}  \\right)^{-1}{\\bf y}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Large-scale kernel methods\n",
    "\n",
    "## 1.1 Nyström approximation\n",
    "\n",
    "The Nyström method approximates a kernel map using a subset of the training data.\n",
    "\n",
    "$${\\bf K} = \\begin{bmatrix} {\\bf K}_{11} & {\\bf K}_{12}\\\\ {\\bf K}_{21} & {\\bf K}_{22} \\end{bmatrix}$$\n",
    "\n",
    "$${\\bf C} = \\begin{bmatrix} {\\bf K}_{11}\\\\ {\\bf K}_{21}\\end{bmatrix}$$\n",
    "\n",
    "$${\\bf \\tilde{K}} = {\\bf C} {\\bf K}_{11}^{-1} {\\bf C}^\\top $$\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.Nystroem.html\n",
    "\n",
    "Let us illustrate this method on the noisy sinc data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "n_data = 10000 # number of data\n",
    "noise_std = 0.05 # noise standard deviation\n",
    "\n",
    "# generate noisy sinc data\n",
    "np.random.seed(seed=0)\n",
    "X = np.random.normal(0,2,n_data).reshape(-1,1) # reshape makes a 2D array\n",
    "noise = np.random.normal(0,noise_std,n_data)\n",
    "y = np.sinc(X[:,0]) + noise # 1D array of size (n_data,)\n",
    "\n",
    "# scatter plot of the data\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.scatter(X[:,0],y,s=20,alpha=0.1)\n",
    "plt.title('Noisy sinc')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a scikit-learn \"Pipeline\" to glue together the different processing steps of the method.  \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import pipeline\n",
    "\n",
    "# parameters\n",
    "n_components = 50 # size of the subset\n",
    "gamma = 1 # kernel parameter\n",
    "\n",
    "# define the feature map\n",
    "feature_map_nystroem = Nystroem(gamma=gamma, n_components=n_components, random_state=0)\n",
    "\n",
    "# split the full kernel ridge regression into approximate feature map + linear regression\n",
    "nystroem_approx_krr = pipeline.Pipeline([(\"feature_map\", feature_map_nystroem),\n",
    "                                        (\"ridge\", LinearRegression())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the pipeline regressor\n",
    "nystroem_approx_krr.fit(X,y)\n",
    "\n",
    "# analyze both parts of the pipeline\n",
    "print(nystroem_approx_krr.named_steps['feature_map'])\n",
    "print(nystroem_approx_krr.named_steps['ridge'])\n",
    "\n",
    "# Test the regressor\n",
    "X_test = np.linspace(-8,8,1000).reshape(-1,1)\n",
    "y_pred = nystroem_approx_krr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices of training subset used to construct the mapping\n",
    "idx = nystroem_approx_krr.named_steps['feature_map'].component_indices_[:n_components]\n",
    "\n",
    "# plot the results\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.scatter(X[:,0],y,s=20,alpha=0.1,label='data')\n",
    "plt.plot(X_test[:,0],y_pred,linewidth=3,color='red',label='KRR approximation (Nystroem)')\n",
    "plt.scatter(X[idx,0],y[idx],s=40,color='yellow',alpha=1,edgecolor='black',label='basis vectors')\n",
    "plt.title('Noisy sinc')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Random Fourier Features\n",
    "\n",
    "The Random Fourier Features method approximates the feature map of an RBF kernel by Monte Carlo approximation of its Fourier transform.\n",
    "\n",
    "We look for a finite dimensional mapping $\\Psi$ such that:\n",
    "$$\\kappa({\\bf x},{\\bf y}) \\approx \\langle \\Psi({\\bf x}), \\Psi({\\bf y})\\rangle$$\n",
    "\n",
    "In the case of the Gaussian kernel, *Bochner’s theorem* shows that a mapping that satisfies this condition on average is\n",
    "\n",
    "$$\\Psi({\\bf x}) = \\sqrt{2} \\cos(\\omega^\\top {\\bf x} + b)$$\n",
    "\n",
    "where b is drawn uniformly from $[0, 2\\pi]$ and $\\omega$ is drawn randomly from $\\mathcal{N}(0, I_d/\\sigma^2)$, where d is the input space dimension. In practice, we will sample $n$ values from the mapping and average out the resulting mapping.\n",
    "\n",
    "This method is implemented in scikit-learn's \"RBFSampler\": http://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.RBFSampler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_approximation import RBFSampler\n",
    "\n",
    "# parameters\n",
    "n_components = 100 # dimension of approximation K11\n",
    "gamma = 1 # kernel parameter\n",
    "\n",
    "# define the feature map\n",
    "feature_map_fourier = RBFSampler(gamma=gamma, n_components=n_components, random_state=0)\n",
    "\n",
    "# split the full kernel ridge regression into approximate feature map + linear regression\n",
    "fourier_approx_krr = pipeline.Pipeline([(\"feature_map\", feature_map_fourier),\n",
    "                                        (\"ridge\", LinearRegression())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the regressor\n",
    "fourier_approx_krr.fit(X,y)\n",
    "\n",
    "# analyze both parts of the pipeline\n",
    "print(fourier_approx_krr.named_steps['feature_map'])\n",
    "print(fourier_approx_krr.named_steps['ridge'])\n",
    "\n",
    "# Test the regressor\n",
    "X_test = np.linspace(-6,6,1000).reshape(-1,1)\n",
    "y_pred = fourier_approx_krr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.scatter(X[:,0],y,s=20,alpha=0.1,label='data')\n",
    "plt.plot(X_test[:,0],y_pred,linewidth=3,color='red',label='KRR approximation (RFF)')\n",
    "plt.title('Noisy sinc')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map = fourier_approx_krr.named_steps['feature_map']\n",
    "\n",
    "omega = feature_map.random_weights_.T # the random weights omega\n",
    "offsets = feature_map.random_offset_.T # the random offsets b\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "# plot the histograms\n",
    "n, bins, patches = ax[0].hist(omega, 50, density=1, facecolor='g', alpha=0.75)\n",
    "ax[0].grid(True)\n",
    "ax[0].set_title(\"Omega\")\n",
    "\n",
    "# the histogram of the data\n",
    "n, bins, patches = ax[1].hist(offsets, 50, density=1, facecolor='g', alpha=0.75)\n",
    "ax[1].grid(True)\n",
    "ax[1].set_title(\"offsets\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Nyström and RFF for classification\n",
    "\n",
    "In the following example we will show how to use RBFSampler and the Nystroem method to approximate the feature map of an RBF kernel.\n",
    "The problem will be classification of the UCI ML hand-written digits dataset using an SVM.\n",
    "Three methods will be compared: a linear SVM in the original space, a linear SVM using the approximate mappings and a kernelized SVM.\n",
    "\n",
    "First, let us load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from time import time\n",
    "\n",
    "# The digits dataset\n",
    "digits = datasets.load_digits(n_class=10)\n",
    "\n",
    "print(\"Size of the data: {}\".format(digits.data.shape))\n",
    "print(\"Size of the targets: {}\".format(digits.target.shape))\n",
    "\n",
    "# Plot some digits\n",
    "images_and_labels = list(zip(digits.images, digits.target))\n",
    "plt.figure(figsize=(10,6))\n",
    "for index, (image, label) in enumerate(images_and_labels[:10]):\n",
    "    plt.subplot(2, 5, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(digits.data)\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, digits.target, \n",
    "                                                    test_size=0.33, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark with linear SVM and full kernel SVM\n",
    "\n",
    "We will use the \"score\" method of the classiier, which returns the mean accuracy.  \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.base.ClassifierMixin.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classifier: a support vector classifier\n",
    "kernel_svm = svm.SVC(gamma=.02)\n",
    "linear_svm = svm.LinearSVC(max_iter=2000, random_state=0)\n",
    "\n",
    "# train and test with linear SVM\n",
    "linear_svm_time = time()\n",
    "linear_svm.fit(X_train, y_train)\n",
    "linear_svm_score = linear_svm.score(X_test, y_test)\n",
    "linear_svm_time = time() - linear_svm_time\n",
    "print(\"Linear SVM training time: %.2f s. Score: %.2f.\"%(linear_svm_time,linear_svm_score))\n",
    "\n",
    "# train and test with kernel SVM\n",
    "kernel_svm_time = time()\n",
    "kernel_svm.fit(X_train, y_train)\n",
    "kernel_svm_score = kernel_svm.score(X_test, y_test)\n",
    "kernel_svm_time = time() - kernel_svm_time\n",
    "print(\"Kernel SVM training time: %.2f s. Score: %.2f.\"%(kernel_svm_time,kernel_svm_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the RFF and Nystroem classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pipeline from kernel approximation and linear svm\n",
    "feature_map_fourier = RBFSampler(gamma=.02, random_state=1)\n",
    "feature_map_nystroem = Nystroem(gamma=.02, random_state=1)\n",
    "\n",
    "fourier_approx_svm = pipeline.Pipeline([(\"feature_map\", feature_map_fourier),\n",
    "                                        (\"svm\", svm.LinearSVC())])\n",
    "\n",
    "nystroem_approx_svm = pipeline.Pipeline([(\"feature_map\", feature_map_nystroem),\n",
    "                                        (\"svm\", svm.LinearSVC())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform regression with different precisions of kernel approximations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define number of components / bases to test\n",
    "sample_sizes = 50 * np.arange(1, 10)\n",
    "\n",
    "fourier_scores = []\n",
    "nystroem_scores = []\n",
    "fourier_times = []\n",
    "nystroem_times = []\n",
    "\n",
    "for D in sample_sizes:\n",
    "    \n",
    "    # set parameters\n",
    "    fourier_approx_svm.set_params(feature_map__n_components=D)\n",
    "    nystroem_approx_svm.set_params(feature_map__n_components=D)\n",
    "    \n",
    "    # train and time\n",
    "    start = time()\n",
    "    nystroem_approx_svm.fit(X_train, y_train)\n",
    "    nystroem_times.append(time() - start)\n",
    "\n",
    "    # train and time\n",
    "    start = time()\n",
    "    fourier_approx_svm.fit(X_train, y_train)\n",
    "    fourier_times.append(time() - start)\n",
    "\n",
    "    # store scores\n",
    "    fourier_scores.append(fourier_approx_svm.score(X_test, y_test))\n",
    "    nystroem_scores.append(nystroem_approx_svm.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot the results:\n",
    "plt.figure(figsize=(15, 6))\n",
    "accuracy = plt.subplot(121)\n",
    "\n",
    "# second y axis for timeings\n",
    "timescale = plt.subplot(122)\n",
    "\n",
    "accuracy.plot(sample_sizes, nystroem_scores, label=\"Nystroem approx. kernel\")\n",
    "timescale.plot(sample_sizes, nystroem_times, '--',\n",
    "               label='Nystroem approx. kernel')\n",
    "\n",
    "accuracy.plot(sample_sizes, fourier_scores, label=\"Fourier approx. kernel\")\n",
    "timescale.plot(sample_sizes, fourier_times, '--',\n",
    "               label='Fourier approx. kernel')\n",
    "\n",
    "# horizontal lines for exact rbf and linear kernels:\n",
    "accuracy.plot([sample_sizes[0], sample_sizes[-1]],\n",
    "              [linear_svm_score, linear_svm_score], label=\"linear svm\")\n",
    "timescale.plot([sample_sizes[0], sample_sizes[-1]],\n",
    "               [linear_svm_time, linear_svm_time], '--', label='linear svm')\n",
    "\n",
    "accuracy.plot([sample_sizes[0], sample_sizes[-1]],\n",
    "              [kernel_svm_score, kernel_svm_score], label=\"rbf svm\")\n",
    "timescale.plot([sample_sizes[0], sample_sizes[-1]],\n",
    "               [kernel_svm_time, kernel_svm_time], '--', label='rbf svm')\n",
    "\n",
    "# vertical line for dataset dimensionality = 64\n",
    "accuracy.plot([64, 64], [0.7, 1], label=\"n_features\")\n",
    "\n",
    "# legends and labels\n",
    "accuracy.set_title(\"Classification accuracy\")\n",
    "timescale.set_title(\"Training times\")\n",
    "accuracy.set_xlim(sample_sizes[0], sample_sizes[-1])\n",
    "accuracy.set_ylim(np.min(fourier_scores), 1)\n",
    "accuracy.set_xlabel(\"Sampling steps = transformed feature dimension\")\n",
    "timescale.set_xlabel(\"Sampling steps = transformed feature dimension\")\n",
    "accuracy.set_ylabel(\"Classification accuracy\")\n",
    "timescale.set_ylabel(\"Training time in seconds\")\n",
    "accuracy.legend(loc='best')\n",
    "timescale.legend(loc='best')\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the decision surface "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the decision surface, projected down to the first\n",
    "# two principal components of the dataset\n",
    "pca = PCA(n_components=8).fit(X_train)\n",
    "\n",
    "X = pca.transform(X_train)\n",
    "\n",
    "# Generate grid along first two principal components\n",
    "multiples = np.arange(-10, 10, 0.5)\n",
    "# steps along first component\n",
    "first = multiples[:, np.newaxis] * pca.components_[0, :]\n",
    "# steps along second component\n",
    "second = multiples[:, np.newaxis] * pca.components_[1, :]\n",
    "# combine\n",
    "grid = first[np.newaxis, :, :] + second[:, np.newaxis, :]\n",
    "flat_grid = grid.reshape(-1, data.shape[1])\n",
    "\n",
    "# title for the plots\n",
    "titles = ['SVC with rbf kernel',\n",
    "          'SVC (linear kernel)\\n with Fourier rbf feature map\\n'\n",
    "          'n_components=100',\n",
    "          'SVC (linear kernel)\\n with Nystroem rbf feature map\\n'\n",
    "          'n_components=100']\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# predict and plot\n",
    "for i, clf in enumerate((kernel_svm, nystroem_approx_svm,\n",
    "                         fourier_approx_svm)):\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    Z = clf.predict(flat_grid)\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(grid.shape[:-1])\n",
    "    plt.contourf(multiples, multiples, Z, cmap=plt.cm.Paired)\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y_train, cmap=plt.cm.Paired,\n",
    "                edgecolors=(0, 0, 0))\n",
    "\n",
    "    plt.title(titles[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: SGDClassifier\n",
    "\n",
    "When the data sets are still too large for batch training we can replace linear learners like Perceptron, Logistic Regression and SVM by `SGDClassifier` which implements stochastic-gradient-descent versions of these batch algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "ppn = SGDClassifier(loss='perceptron')\n",
    "lr = SGDClassifier(loss='log')\n",
    "svm = SGDClassifier(loss='hinge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Nyström and RFF for regression\n",
    "\n",
    "We will now compare both methods for the kernel ridge regression task on the KIN40K data set.\n",
    "\n",
    "### KIN40K data set\n",
    "\n",
    "- Forward kinematics of an 8-link all-revolute robot arm.\n",
    "- 40000 examples, each consisting of an 8-dimensional input vector and a scalar output.\n",
    "- KIN40K was generated with maximum nonlinearity and little noise.\n",
    "\n",
    "![PUMA 560](https://i.imgur.com/qCN3Jk8.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "\n",
    "kin40k = loadmat('data/kin40k.mat')\n",
    "\n",
    "X = kin40k['X']\n",
    "y = kin40k['y']\n",
    "\n",
    "print('Data shape:\\nX: %s\\ny: %s\\n'%(X.shape,y.shape))\n",
    "\n",
    "# apply scaling with coefficients learned by Gaussian processes\n",
    "loghyper = loadmat('data/kin40k_loghyper.mat')\n",
    "\n",
    "# convert log values to normal range\n",
    "scalers = np.exp(loghyper['loghyper'][0:8]).ravel()\n",
    "\n",
    "# divide features by scaling coefficients\n",
    "X /= scalers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# EDA: 1D inputs vs output\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "for i in np.arange(X.shape[1]):\n",
    "    plt.subplot(2,4,i+1)\n",
    "    plt.scatter(X[:,i],y,alpha=0.1)\n",
    "    \n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair plots\n",
    "\n",
    "import seaborn as sns; sns.set(style=\"ticks\", color_codes=True)\n",
    "\n",
    "start = time()\n",
    "df = pd.DataFrame(data=X[:5000,:])\n",
    "g = sns.pairplot(df)\n",
    "my_time = time() - start\n",
    "\n",
    "print(my_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No structure is apparent.  \n",
    "The KIN40K data set represents a difficult nonlinear regression problem.  \n",
    "Let us train the predictors on the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.75, random_state=42)\n",
    "    \n",
    "print('Training data shape:\\nX_train: %s\\ny_train: %s\\n'%(X_train.shape,y_train.shape))\n",
    "print('Test data shape:\\nX_test: %s\\ny_test: %s'%(X_test.shape,y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard ML techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try some standard regression methods\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Linear regression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "y_pred_lin_reg = lin_reg.predict(X_test)\n",
    "\n",
    "# K-nearest neighbor regression\n",
    "neigh= KNeighborsRegressor(n_neighbors=5)\n",
    "neigh.fit(X_train, y_train)\n",
    "y_pred_neigh = neigh.predict(X_test)\n",
    "\n",
    "# calculate mean square error\n",
    "mse_lin_reg = mean_squared_error(y_test, y_pred_lin_reg)\n",
    "mse_neigh = mean_squared_error(y_test, y_pred_neigh)\n",
    "\n",
    "# print MSE values in standard scale and dB scale\n",
    "print('MSE for Linear Regression: %.4f'%mse_lin_reg)\n",
    "print('MSE for K-Nearest Neighbors: %.4f\\n'%mse_neigh)\n",
    "print('MSE (dB) for Linear Regression: %.4f'%(10*np.log10(mse_lin_reg)))\n",
    "print('MSE (dB) for K-Nearest Neighbors: %.4f'%(10*np.log10(mse_neigh)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Kernel Ridge Regression with Nyström and RFF transforms\n",
    "\n",
    "Apply approximate kernel ridge regression to the KIN40K data.\n",
    "Compare the results of the Nyström and RFF approximations.\n",
    "\n",
    "A good starting value for the $\\gamma$ parameter of the RBF kernel is $\\gamma=1/2$. Find a fitting value for the parameter \"n_components\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: change the following lines to properly define the regressors\n",
    "dummy = LinearRegression()\n",
    "nystroem_approx_krr = dummy\n",
    "fourier_approx_krr = dummy\n",
    "\n",
    "# The next lines should not be changed:\n",
    "\n",
    "# train the regressors\n",
    "nystroem_approx_krr.fit(X_train,y_train)\n",
    "fourier_approx_krr.fit(X_train,y_train)\n",
    "\n",
    "# test the regressors\n",
    "y_pred_nystroem = nystroem_approx_krr.predict(X_test)\n",
    "y_pred_fourier = fourier_approx_krr.predict(X_test)\n",
    "\n",
    "# calculate mean square error\n",
    "mse_nystroem = mean_squared_error(y_test, y_pred_nystroem)\n",
    "mse_fourier = mean_squared_error(y_test, y_pred_fourier)\n",
    "\n",
    "# print MSE values in standard scale and dB scale\n",
    "print('MSE for Nyström: %.4f'%mse_nystroem)\n",
    "print('MSE for RFF: %.4f\\n'%mse_fourier)\n",
    "print('MSE (dB) for Nyström: %.4f'%(10*np.log10(mse_nystroem)))\n",
    "print('MSE (dB) for RFF: %.4f'%(10*np.log10(mse_fourier)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Online and adaptive metods\n",
    "\n",
    "## 2.1 Introduction: Perceptron\n",
    "\n",
    "As an introductory example we will review the perceptron algorithm, which is based on a simple online learning rule.\n",
    "This is a classification method, and we will apply it on the Iris data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select setosa and versicolor\n",
    "y = y[0:100]\n",
    "y = np.where(y == 0, -1, 1)\n",
    "\n",
    "# extract sepal length and petal length\n",
    "X = X[0:100, [0, 2]]\n",
    "\n",
    "# plot data\n",
    "plt.scatter(X[:50, 0], X[:50, 1],\n",
    "            color='red', marker='o', label='setosa')\n",
    "plt.scatter(X[50:100, 0], X[50:100, 1],\n",
    "            color='blue', marker='x', label='versicolor')\n",
    "\n",
    "plt.xlabel('sepal length [cm]')\n",
    "plt.ylabel('petal length [cm]')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(object):\n",
    "    \"\"\"Perceptron classifier.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    eta : float\n",
    "        Learning rate (between 0.0 and 1.0)\n",
    "    n_iter : int\n",
    "        Passes over the training dataset.\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    w_ : 1d-array\n",
    "        Weights after fitting.\n",
    "    errors_ : list\n",
    "        Number of misclassifications (updates) in each epoch.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, eta=0.01, n_iter=10):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit all training data\"\"\"\n",
    "        self.w_ = np.zeros(1 + X.shape[1])\n",
    "        self.errors_ = []\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            errors = 0\n",
    "            for xi, target in zip(X, y):\n",
    "                errori = self.fit_single(xi,target)\n",
    "                errors += errori\n",
    "            self.errors_.append(errors)\n",
    "        return self\n",
    "    \n",
    "    def init_attr(self, input_dim):\n",
    "        \"\"\"Initialize attributes.\"\"\"\n",
    "        self.w_ = np.zeros(1 + input_dim)\n",
    "        \n",
    "    \n",
    "    def fit_single(self,x,y):\n",
    "        \"\"\"Fit a single training datum\"\"\"\n",
    "        update = self.eta * (y - self.predict(x))\n",
    "        self.w_[1:] += update * x\n",
    "        self.w_[0] += update\n",
    "        \n",
    "        error = int(update != 0.0)\n",
    "        \n",
    "        return error\n",
    "        \n",
    "    def net_input(self, X):\n",
    "        \"\"\"Calculate net input\"\"\"\n",
    "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step\"\"\"\n",
    "        return np.where(self.net_input(X) >= 0.0, 1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the perceptron over 10 epochs on all training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=0)\n",
    "\n",
    "ppn = Perceptron(eta=0.1, n_iter=10)\n",
    "\n",
    "ppn.fit(X, y)\n",
    "\n",
    "# test on training data\n",
    "y_pred = ppn.predict(X)\n",
    "\n",
    "# training error\n",
    "print('Misclassified samples: %d' % (y != y_pred).sum())\n",
    "\n",
    "plt.plot(range(1, len(ppn.errors_) + 1), ppn.errors_, marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Number of updates')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us analyze the errors committed in each individual iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 10\n",
    "ppn = Perceptron(eta=0.1, n_iter=n_iter)\n",
    "ppn.init_attr(input_dim=X.shape[1])\n",
    "\n",
    "errors_all = []\n",
    "\n",
    "print(X_test.shape)\n",
    "\n",
    "for _ in range(n_iter):\n",
    "    for xi, target in zip(X, y):\n",
    "        # run fit_single() instead of fit()\n",
    "        ppn.fit_single(xi,target)\n",
    "        \n",
    "        y_pred = ppn.predict(X)\n",
    "        errors = (y != y_pred).sum()\n",
    "        \n",
    "        errors_all.append(errors)\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(range(1, n_iter*X.shape[0] + 1), errors_all, marker='o')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('errors')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Kernel Least-Mean Squares algorithm\n",
    "\n",
    "We now build the KLMS algorithm using the structure of the perceptron algorithm.\n",
    "Take into account that KLMS is a regression algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "class KLMS(object):\n",
    "    \"\"\"Kernel Least Mean Squares algorithm\"\"\"\n",
    "    def __init__(self, mu=0.5, kernel=RBF()):\n",
    "        self.mu = mu\n",
    "        self.kernel = kernel\n",
    "        \n",
    "    def init_attr(self, input_dim):\n",
    "        \"\"\"Initialize attributes\"\"\"\n",
    "        self.alpha_ = np.empty((0,), float)\n",
    "        self.dict = np.empty((0,input_dim), float)\n",
    "    \n",
    "    def fit_single(self,x,y):\n",
    "        \"\"\"Train for one datum\"\"\"\n",
    "        if not hasattr(self, 'dict'):\n",
    "            # initialize\n",
    "            self.init_attr(x.shape[0])\n",
    "\n",
    "        err = y - self.predict(x)\n",
    "        update = self.mu * err\n",
    "        self.alpha_ = np.hstack((self.alpha_,update))\n",
    "        self.dict = np.vstack((self.dict,x))\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict outputs\"\"\"\n",
    "        K = self.kernel.__call__(X,self.dict)\n",
    "        return np.dot(K,self.alpha_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test KLMS on the noisy sinc data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate noisy sinc data\n",
    "n_data = 500\n",
    "np.random.seed(seed=0)\n",
    "X = np.random.normal(0,2,n_data).reshape(-1,1)\n",
    "noise = np.random.normal(0,0.05,n_data)\n",
    "y = np.sinc(X[:,0]) + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "klms = KLMS(mu=0.1, kernel=RBF(length_scale=.5))\n",
    "\n",
    "mse_all = []\n",
    "\n",
    "start = time()\n",
    "for xi, target in zip(X, y):\n",
    "    klms.fit_single(xi,target)\n",
    "\n",
    "    y_pred = klms.predict(X)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "\n",
    "    mse_all.append(mse)\n",
    "klms_time = time() - start\n",
    "\n",
    "print('Elapsed time: %.2f s.'%klms_time)\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(range(1, X.shape[0] + 1), 10*np.log10(mse_all))\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('MSE (dB)')\n",
    "plt.title('Learning curve')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the regressor\n",
    "X_test = np.linspace(-6,6,1000).reshape(-1,1)\n",
    "y_pred = klms.predict(X_test)\n",
    "\n",
    "# plot the results\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.scatter(X[:,0],y,s=20,alpha=0.5)\n",
    "plt.plot(X_test[:,0],y_pred,linewidth=2,color='red')\n",
    "plt.title('Noisy sinc')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Coherence Criterion: Kernel Normalized Least Mean Squares\n",
    "\n",
    "In order to reduce the memory size, we will now implement the KNLMS algorithm, which slows down the dictionary growth by using a coherence-criterion based mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNLMS(object):\n",
    "    \"\"\"Kernel Normalised Least Mean Squares algorithm\"\"\"\n",
    "    def __init__(self, eta=0.5, mu0=0.8, eps=1E-2, kernel=RBF()):\n",
    "        self.eta = eta # step size\n",
    "        self.mu0 = mu0 # coherence threshold\n",
    "        self.eps = eps # regularization\n",
    "        self.kernel = kernel\n",
    "        \n",
    "    def init_attr(self, input_dim):\n",
    "        \"\"\"Initialize attributes\"\"\"\n",
    "        self.alpha_ = np.empty((0,), float)\n",
    "        self.dict = np.empty((0,input_dim), float)\n",
    "    \n",
    "    def fit_single(self,x,y):\n",
    "        \"\"\"Train for one datum\"\"\"\n",
    "        if not hasattr(self, 'dict'):\n",
    "            # initialize\n",
    "            self.init_attr(x.shape[0])\n",
    "            \n",
    "            self.dict = np.vstack((self.dict,x))\n",
    "            self.alpha_ = np.hstack((self.alpha_,0))        \n",
    "        else:\n",
    "            k = self.kernel.__call__(x,self.dict)\n",
    "            #::GMG::la clave es el umbral de inclusión en el diccionario\n",
    "            #::nota::recordar \n",
    "            if np.amax(k) < self.mu0:\n",
    "                self.dict = np.vstack((self.dict,x))\n",
    "                self.alpha_ = np.hstack((self.alpha_,0))\n",
    "        \n",
    "        k = np.squeeze(np.asarray(self.kernel.__call__(x,self.dict)))\n",
    "        err = y - self.predict(x)\n",
    "        \n",
    "        # update alpha\n",
    "        #::nota::en realidad actualiza todo los alphas\n",
    "        self.alpha_ = self.alpha_ + self.eta / (self.eps + np.dot(k,k)) * err * k;\n",
    "        self.alpha_ = self.alpha_.ravel()\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict outputs\"\"\"\n",
    "        K = self.kernel.__call__(X,self.dict)\n",
    "        return np.dot(K,self.alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#::nota::se guardan 32 bases después de 500 valores de muestra\n",
    "#::nota::ver el ejemplo de cybercamp 2018 con EKG (!!)\n",
    "knlms = KNLMS(eta=0.5, mu0=0.9, kernel=RBF(length_scale=.5))\n",
    "\n",
    "mse_all = []\n",
    "\n",
    "start = time()\n",
    "for xi, target in zip(X, y):\n",
    "    knlms.fit_single(xi,target)\n",
    "\n",
    "    y_pred = knlms.predict(X)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "\n",
    "    mse_all.append(mse)\n",
    "knlms_time = time() - start\n",
    "\n",
    "print('Elapsed time: %.2f s.'%knlms_time)\n",
    "\n",
    "print('Final dictionary size: %d bases.'%knlms.dict.shape[0])\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(range(1, X.shape[0] + 1), 10*np.log10(mse_all))\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('MSE (dB)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the regressor\n",
    "X_test = np.linspace(-6,6,1000).reshape(-1,1)\n",
    "y_pred = klms.predict(X_test)\n",
    "\n",
    "# plot the results\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.scatter(X[:,0],y,s=20,alpha=0.5)\n",
    "plt.plot(X_test[:,0],y_pred,linewidth=2,color='red')\n",
    "plt.scatter(knlms.dict,np.zeros(knlms.dict.shape[0],),s=60,color='yellow',edgecolor='black')\n",
    "plt.title('Noisy sinc')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Nonlinear channel equalization\n",
    "---\n",
    "\n",
    "Reference: https://github.com/steven2358/kafbox/blob/master/demo/literature/liu2010kernel/fig2_12.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#::GMG::es la implementación de s30/33\n",
    "channel_data = loadmat('data/nonlinear_channel_eq.mat')\n",
    "\n",
    "x = channel_data['x']\n",
    "y = channel_data['y']\n",
    "\n",
    "n_samples = len(x)\n",
    "\n",
    "print('Data shape:\\nx: %s\\ny: %s\\n'%(x.shape,y.shape))\n",
    "\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.plot(x[0:100],'-o')\n",
    "plt.title('channel output (x)')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.plot(y[0:100],'o')\n",
    "plt.title('channel input (y)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add embedding to input\n",
    "\n",
    "embedding = 5\n",
    "\n",
    "# time embedding\n",
    "X = np.zeros((n_samples,embedding))\n",
    "for i in np.arange(0,embedding):\n",
    "    X[i:,i] = x[0:n_samples-i].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train and test size\n",
    "n_tr = 2000\n",
    "n_te = 50\n",
    "\n",
    "X_train = X[0:n_tr]\n",
    "y_train = y[0:n_tr]\n",
    "\n",
    "X_test = X[n_tr:n_tr+n_te]\n",
    "y_test = y[n_tr:n_tr+n_te]\n",
    "\n",
    "# define the predictors\n",
    "\n",
    "knlms = KNLMS(eta=0.5, mu0=0.5, kernel=RBF(length_scale=1))\n",
    "klms = KLMS(mu=0.5, kernel=RBF(length_scale=1))\n",
    "\n",
    "predictors = [knlms, klms]\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "\n",
    "for predictor in predictors:\n",
    "\n",
    "    mse_all = []\n",
    "\n",
    "    pname = predictor.__class__.__name__\n",
    "    print('Training %s...'%pname)\n",
    "\n",
    "    start = time()\n",
    "    for xi, yi in zip(X_train, y_train):\n",
    "\n",
    "        # train on new datum\n",
    "        predictor.fit_single(xi,yi)\n",
    "\n",
    "        # predict output\n",
    "        y_pred = predictor.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mse_all.append(mse)    \n",
    "\n",
    "    predictor_time = time() - start\n",
    "\n",
    "    print('Elapsed time: %.2f s.'%predictor_time)\n",
    "https://datasciencehub.ifca.es/user/yerartdev/notebooks/work/m1970/lab/P03_kernel_large_scale_online.ipynb#Exercise:-Fixed-Budget-Kernel-Least-Mean-Squares\n",
    "    print('Final dictionary size: %d bases.\\n'%predictor.dict.shape[0])\n",
    "    \n",
    "    plt.plot(range(1, len(mse_all) + 1), 10*np.log10(mse_all), label=pname)\n",
    "\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('MSE (dB)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Fixed-Budget Kernel Least Mean Squares\n",
    "---\n",
    "\n",
    "The dictionary size can be limited by curbing its growth, which is what the coherence criterion is used for.\n",
    "An alternative way of limiting the dictionary size is by *removing* bases that are less relevant.\n",
    "In practice, we will start removing bases from the dictionary (and their corresponding coefficients from alpha) once the dictionary reaches a predefined size.\n",
    "\n",
    "In this exercise you will implement a modified KLMS algorithm that includes dictionary pruning:\n",
    "1. Modify the KLMS algorithm: In `__init__()`, add a parameter `max_bases` that indicates the maximum size of the dictionary.\n",
    "2. In the `fit()` method, add a check to see if the dictionary size has reached the maximum. If this is the case, remove the dictionary element that corresponds to the coefficient of alpha with the lowest absolute value. Prune the dictionary element and the corresponding alpha coefficient.\n",
    "3. Obtain the learning curve of this algorithm to KLMS and KNLMS. \n",
    "4. Describe the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. and 2. Implement the modified KLMS algorithm\n",
    "\n",
    "class SFBKLMS(object):\n",
    "    \"\"\"Simple Fixed Budget Kernel Least Mean Squares algorithm\"\"\"\n",
    "    def __init__(self, mu=0.5, kernel=RBF()):\n",
    "        self.mu = mu\n",
    "        self.kernel = kernel\n",
    "        \n",
    "    def init_attr(self, input_dim):\n",
    "        \"\"\"Initialize attributes\"\"\"\n",
    "        self.alpha_ = np.empty((0,), float)\n",
    "        self.dict = np.empty((0,input_dim), float)\n",
    "    \n",
    "    def fit_single(self,x,y):\n",
    "        \"\"\"Train for one datum\"\"\"\n",
    "        if not hasattr(self, 'dict'):\n",
    "            # initialize\n",
    "            self.init_attr(x.shape[0])\n",
    "\n",
    "        err = y - self.predict(x)\n",
    "        update = self.mu * err\n",
    "        self.alpha_ = np.hstack((self.alpha_,update))\n",
    "        self.dict = np.vstack((self.dict,x))\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict outputs\"\"\"\n",
    "        K = self.kernel.__call__(X,self.dict)\n",
    "        return np.dot(K,self.alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. If required, modify the  algorithm parameters. Obtain the learning curves.\n",
    "\n",
    "# define the predictors\n",
    "\n",
    "knlms = KNLMS(eta=0.5, mu0=0.5, kernel=RBF(length_scale=1))\n",
    "klms = KLMS(mu=0.5, kernel=RBF(length_scale=1))\n",
    "\n",
    "# add this parameter to the method\n",
    "max_bases = 500\n",
    "sfbklms = SFBKLMS(mu=0.5, kernel=RBF(length_scale=1))\n",
    "\n",
    "predictors = [knlms, klms, sfbklms]\n",
    "#predictors = [sfbklms]\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "\n",
    "for predictor in predictors:\n",
    "\n",
    "    mse_all = []\n",
    "\n",
    "    pname = predictor.__class__.__name__\n",
    "    print('Training %s...'%pname)\n",
    "\n",
    "    start = time()\n",
    "    for xi, yi in zip(X_train, y_train):\n",
    "\n",
    "        # train on new datum\n",
    "        predictor.fit_single(xi,yi)\n",
    "\n",
    "        # predict output\n",
    "        y_pred = predictor.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mse_all.append(mse)    \n",
    "\n",
    "    predictor_time = time() - start\n",
    "\n",
    "    print('Elapsed time: %.2f s.'%predictor_time)\n",
    "\n",
    "    print('Final dictionary size: %d bases.\\n'%predictor.dict.shape[0])\n",
    "    \n",
    "    plt.plot(range(1, len(mse_all) + 1), 10*np.log10(mse_all), label=pname)\n",
    "\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('MSE (dB)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Describe the result of the modified KLMS algorithm:\n",
    "# Did you find a parameter setting that produces good results?\n",
    "# Do the results hold if you raise n_tr?\n",
    "# Is this algorithm an improvement of KLMS?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources\n",
    "\n",
    "- [Python Machine Learning by Sebastian Raschka](https://github.com/rasbt/python-machine-learning-bookk). MIT License.\n",
    "- [scikit-learn Examples](http://scikit-learn.org/stable/auto_examples/). BSD License.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
